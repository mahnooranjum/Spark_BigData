{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c61bbdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ec807e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb7feec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/23 21:14:31 WARN Utils: Your hostname, maq-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "21/11/23 21:14:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/maq/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/11/23 21:14:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Basic').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca5472c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"/home/maq/Datasets/customer_churn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b66463fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6db5d4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Total_Purchase: string (nullable = true)\n",
      " |-- Account_Manager: string (nullable = true)\n",
      " |-- Years: string (nullable = true)\n",
      " |-- Num_Sites: string (nullable = true)\n",
      " |-- Onboard_date: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Churn: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"/home/maq/Datasets/customer_churn.csv\", header = True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f068d3c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Names',\n",
       " 'Age',\n",
       " 'Total_Purchase',\n",
       " 'Account_Manager',\n",
       " 'Years',\n",
       " 'Num_Sites',\n",
       " 'Onboard_date',\n",
       " 'Location',\n",
       " 'Company',\n",
       " 'Churn']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62a00544",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, Names: string, Age: string, Total_Purchase: string, Account_Manager: string, Years: string, Num_Sites: string, Onboard_date: string, Location: string, Company: string, Churn: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f625cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (StructField, \n",
    "                               StringType,\n",
    "                               IntegerType,\n",
    "                               FloatType, \n",
    "                               DoubleType,\n",
    "                               DecimalType,\n",
    "                               BooleanType,\n",
    "                               StructType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6fd558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def schemaBuilder(columns):                \n",
    "    data_schema = []\n",
    "    for i in columns:\n",
    "        whatis = input(i + \" is int, float, double, decimal, bool, string :\")\n",
    "        if (whatis == 'float'):\n",
    "            dataType = FloatType()\n",
    "        elif (whatis == 'bool'):\n",
    "            dataType = BooleanType()\n",
    "        elif (whatis == 'double'):\n",
    "            dataType = DoubleType()\n",
    "        elif (whatis == 'int'):\n",
    "            dataType = IntegerType()\n",
    "        elif (whatis == 'decimal'):\n",
    "            dataType = DecimalType()\n",
    "        else:\n",
    "            dataType = StringType()\n",
    "        \n",
    "        data_schema.append(StructField(i, dataType, True))\n",
    "        \n",
    "    return data_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98b2e41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Names is int, float, double, decimal, bool, string :string\n",
      "Age is int, float, double, decimal, bool, string :int\n",
      "Total_Purchase is int, float, double, decimal, bool, string :float\n",
      "Account_Manager is int, float, double, decimal, bool, string :string\n",
      "Years is int, float, double, decimal, bool, string :float\n",
      "Num_Sites is int, float, double, decimal, bool, string :float\n",
      "Onboard_date is int, float, double, decimal, bool, string :string\n",
      "Location is int, float, double, decimal, bool, string :string\n",
      "Company is int, float, double, decimal, bool, string :string\n",
      "Churn is int, float, double, decimal, bool, string :string\n"
     ]
    }
   ],
   "source": [
    "data_schema = schemaBuilder(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66b48b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_struct = StructType(fields = data_schema)\n",
    "\n",
    "# df = spark.read.json('persons.json', schema = final_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efcb28fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Total_Purchase: float (nullable = true)\n",
      " |-- Account_Manager: string (nullable = true)\n",
      " |-- Years: float (nullable = true)\n",
      " |-- Num_Sites: float (nullable = true)\n",
      " |-- Onboard_date: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Churn: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"/home/maq/Datasets/customer_churn.csv\", header = True, schema = final_struct)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264e853c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
